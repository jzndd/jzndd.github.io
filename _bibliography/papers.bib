---
---

@string{aps = {American Physical Society,}}

@inproceedings{chen2024TeVIR,
  abbr={TSMC},
  title={TeViR: Text-to-Video Reward with Diffusion Models for Efficient Reinforcement Learning}, 
  author={Chen, Yuhui and Li, Haoran and Jiang, Zhennan and Wen, Haowei and Zhao, Dongbin},
  booktitle={IEEE Transactions on Systems Man Cybernetics-Systems},
  abstract={Developing scalable and generalizable reward engineering for reinforcement learning (RL) is crucial for creating general-purpose agents, especially in the challenging domain of robotic manipulation. While recent advances in reward engineering with Vision-Language Models (VLMs) have shown promise, their sparse reward nature significantly limits sample efficiency. This paper introduces TeViR, a novel method that leverages a pre-trained text-to-video diffusion model to generate dense rewards by comparing the predicted image sequence with current observations. Experimental results across 11 complex robotic tasks demonstrate that TeViR outperforms traditional methods leveraging sparse rewards and other state-of-the-art (SOTA) methods, achieving better sample efficiency and performance without ground truth environmental rewards. TeViRâ€™s ability to efficiently guide agents in complex environments highlights its potential to advance reinforcement learning applications in robotic manipulation.},
  year={2024},
  primaryClass={cs.LG},
  selected={true},
  url={https://arxiv.org/abs/2505.19769},
  preview={tevir.png},
  arxiv={2505.19769},
  status={published}
}

@inproceedings{li2024generalizingconsistencypolicyvisual,
  abbr={NeurIPS 2024 Poster},
  title={Generalizing Consistency Policy to Visual RL with Prioritized Proximal Experience Regularization}, 
  author={Li, Haoran and Jiang, Zhennan and Chen, Yuhui and Zhao, Dongbin},
  booktitle={The 38th Annual Conference on Neural Information Processing Systems, {NIPS}},
  abstract={With high-dimensional state spaces, visual reinforcement learning (RL) faces significant challenges in exploitation and exploration, resulting in low sample efficiency and training stability. As a time-efficient diffusion model, although consistency models have been validated in online state-based RL, it is still an open question whether it can be extended to visual RL. In this paper, we investigate the impact of non-stationary distribution and the actor-critic framework on consistency policy in online RL, and find that consistency policy was unstable during the training, especially in visual RL with the high-dimensional state space. To this end, we suggest sample-based entropy regularization to stabilize the policy training, and propose a consistency policy with prioritized proximal experience regularization (CP3ER) to improve sample efficiency. CP3ER achieves new state-of-the-art (SOTA) performance in 21 tasks across DeepMind control suite and Meta-world. To our knowledge, CP3ER is the first method to apply diffusion/consistency models to visual RL and demonstrates the potential of consistency models in visual RL.},
  month={Sept},
  year={2024},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2410.00051}, 
  selected={true},
  html={https://jzndd.github.io/CP3ER-Page/},
  arxiv={2410.00051},
  code={https://github.com/jzndd/CP3ER},
  pdf={cp3er.pdf},
  preview={cp3er.png},
  status={published}
}

@unpublished{lei2025RL-100,
  abbr={Under Review},
  title={RL-100: Performant Robotic Manipulation with Real-World Reinforcement Learning},
  author={Lei*, Kun and Li*, Huanyu and Yu*, Dongjie and Wei*, Zhenyu and Guo, Lingxiao and Jiang, Zhennan and Wang, Ziyu and Liang, Shiyu and Xu, Huazhe},
  journal={Under Review},
  abstract={Real-world robotic manipulation in homes and factories demands reliability, efficiency, and robustness that approach or surpass skilled human operators. We present RL-100, a real-world reinforcement learning training framework built on diffusion visuomotor policies trained bu supervised learning. RL-100 introduces a three-stage pipeline. First, imitation learning leverages human priors. Second, iterative offline reinforcement learning uses an Offline Policy Evaluation procedure, abbreviated OPE, to gate PPO-style updates that are applied in the denoising process for conservative and reliable improvement. Third, online reinforcement learning eliminates residual failure modes. An additional lightweight consistency distillation head compresses the multi-step sampling process in diffusion into a single-step policy, enabling high-frequency control with an order-of-magnitude reduction in latency while preserving task performance. The framework is task-, embodiment-, and representation-agnostic and supports both 3D point clouds and 2D RGB inputs, a variety of robot platforms, and both single-step and action-chunk policies. We evaluate RL-100 on seven real-robot tasks spanning dynamic rigid-body control, such as Push-T and Agile Bowling, fluids and granula pouring, deformable cloth folding, precise dexterous unscrewing, and multi-stage orange juicing. RL-100 attains 100% success across evaluated trials for a total of 900 out of 900 episodes, including up to 250 out of 250 consecutive trials on one task. The method achieves near-human teleoperation or better time efficiency and demonstrates multi-hour robustness with uninterrupted operation lasting up to two hours. The resulting policies generalize zero-shot to novel dynamics with an average success of 92.5% and adapt in a few-shot fashion to significant task variations, reaching an average of 86.7% after one to three hours of additional training. These results suggest a practical path to deployment- ready robot learning by starting from human priors, aligning training objectives with human-grounded metrics, and reliably extending performance beyond human demonstrations},
  year={2025},
  month={Oct},
  primaryClass={cs.LG},
  selected={true},
  preview={RL-100.png},
  pdf={RL-100.pdf},
  html={https://lei-kun.github.io/RL-100/},
  status={preprint}
}


@unpublished{jiang2025world4rl,
  abbr={Under Review},
  title={World4RL: Diffusion World Models for Policy Refinement with Reinforcement Learning for Robotic Manipulation}, 
  author={Jiang, Zhennan and Liu, Kai and Qin, Yuxin and Tian, Shuai and Zheng, Yupeng and Zhou, Mingcai and Yu, Chao and Li, Haoran and Zhao, Dongbin},
  journal={Under Review},
  abstract={Robotic manipulation policies are commonly initialized through imitation learning, but their performance is limited by the scarcity and narrow coverage of expert data. Reinforcement learning can refine polices to alleviate this limitation, yet real-robot training is costly and unsafe, while training in simulators suffers from the sim-to-real gap. Recent advances in generative models have demonstrated remarkable capabilities in real-world simulation, with diffusion models in particular excelling at generation. This raises the question of how diffusion model-based world models can be combined to enhance pre-trained policies in robotic manipulation. In this work, we propose World4RL, a framework that employs diffusion-based world models as high-fidelity simulators to refine pre-trained policies entirely in imagined environments for robotic manipulation. Unlike prior works that primarily employ world models for planning, our framework enables direct end-to-end policy optimization. World4RL is designed around two principles: pre-training a diffusion world model that captures diverse dynamics on multi-task datasets and refining policies entirely within a frozen world model to avoid online real-world interactions. We further design a two-hot action encoding scheme tailored for robotic manipulation and adopt diffusion backbones to improve modeling fidelity. Extensive simulation and real-world experiments demonstrate that World4RL provides high-fidelity environment modeling and enables consistent policy refinement, yielding significantly higher success rates compared to imitation learning and other baselines.},
  year={2025},
  primaryClass={cs.LG},
  selected={true},
  preview={world4rl.png},
  url={https://arxiv.org/abs/2509.19080},
  arxiv={2509.19080},
  pdf={world4rl.pdf},
  html={https://world4rl.github.io/},
  status={preprint}
}

@unpublished{zhang2025marssep,
  abbr={Under Review},
  title={MARS-Sep: Multimodal-Aligned Reinforced Sound Separation}, 
  author={Zhang, Zihan and Cheng, Xize and Jiang, Zhennan and Fu, Dongjie and Chen, Jingyuan and Zhao, Zhou and Jin, Tao},
  journal={Under Review},
  abstract={Universal sound separation faces a fundamental misalignment: models optimized for low-level signal metrics often produce semantically contaminated outputs, failing to suppress perceptually salient interference from acoustically similar sources. To bridge this gap, we introduce MARS-Sep, a reinforcement learning framework that reformulates separation as decision making. Instead of simply regressing ground-truth masks, MARS-Sep learns a factorized Beta mask policy that is optimized by a clipped trust-region surrogate with entropy regularization and group-relative advantage normalization. Concretely, we sample masks from a frozen old policy, reconstruct waveforms, and update the current policy using clipped importance ratios-yielding substantially more stable and sample-efficient learning. Multimodal rewards, derived from an audio-text-vision encoder, directly incentivize semantic consistency with query prompts. We further propose a progressive alignment scheme to fine-tune this encoder, boosting its cross-modal discriminability and improving reward faithfulness. Extensive experiments on multiple benchmarks demonstrate consistent gains in Text-, Audio-, and Image-Queried separation, with notable improvements in signal metrics and semantic quality.},
  year={2025},
  primaryClass={cs.LG},
  selected={true},
  preview={mars-sep.png},
  url={https://arxiv.org/abs/2510.10509},
  arxiv={2510.10509},
  pdf={MARS_Sep.pdf},
  html={https://mars-sep.github.io/},
  status={preprint}
}
